# backend/services/chat_service.py

import asyncio
from typing import AsyncGenerator, Optional, List, Dict

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

from backend.data.topics import TOPICS
from backend.llm_client.base import LLMClient
from backend.utils.prompt_loader import load_prompt
from backend.utils.text_tools import strip_control_markers, parse_control_flags
from backend.services.model2_service import Model2Service
from backend.services.model3_service import Model3Service
from backend.services.db_history_manager import DatabaseHistoryManager
from backend.db.models import TraitProfile, Session


class ChatService:
    """
    ChatServiceï¼šè´Ÿè´£ä¸‰å±‚é€»è¾‘çš„ç¼–æ’ä¸å¯¹æ¥ï¼š
    - model1ï¼šå¯¹è¯ï¼ˆæœ¬ç±»å†…é€šè¿‡ llm å®ç°ï¼‰
    - model2ï¼šè§‚å¿µåˆ†æ + å¯¹è¯å»ºè®®ï¼ˆModel2Serviceï¼‰
    - model3ï¼šç‰¹è´¨åˆ†æï¼ˆModel3Serviceï¼‰

    å¯¹å¤–æš´éœ²ä¸€ä¸ªç»Ÿä¸€çš„æµå¼æ¥å£ï¼š
    async def stream_response(...) -> AsyncGenerator[dict, None]
    """

    def __init__(self, llm: LLMClient):
        self.llm = llm
        self.model2 = Model2Service(llm)
        self.model3 = Model3Service(llm)

    # ------------------------------------------------------
    # è¯»å–ç”¨æˆ·å½“å‰ trait
    # ------------------------------------------------------
    async def _load_trait_context(
        self,
        db: AsyncSession,
        user_id: int,
    ) -> tuple[str, str]:
        """
        ä» TraitProfile è¡¨ä¸­åŠ è½½è¯¥ç”¨æˆ·çš„é•¿æœŸç‰¹è´¨ summary å’Œ full_reportã€‚
        å¦‚æœæ²¡æœ‰è®°å½•ï¼Œåˆ™è¿”å› ('', '')ã€‚
        """
        result = await db.execute(
            select(TraitProfile).where(TraitProfile.user_id == user_id)
        )
        profile = result.scalar_one_or_none()

        if not profile:
            return "", ""

        return str(profile.summary or ""), str(profile.full_report or "")

    # ------------------------------------------------------
    # åå°å¼‚æ­¥ç”ŸæˆæŠ¥å‘Š
    # ------------------------------------------------------
    async def _generate_report_background(
        self,
        session_id: str,
        mode: int,
        topic_id: Optional[int],
        db: AsyncSession,
        trait_summary: str,
        trait_profile: str,
    ):
        """
        åå°ä»»åŠ¡ï¼šç”Ÿæˆè§‚å¿µæŠ¥å‘Šå¹¶æ›´æ–°æ•°æ®åº“
        """
        try:
            # è·å–å®Œæ•´å†å²
            result = await db.execute(
                select(Session).where(Session.id == session_id)
            )
            session = result.scalar_one_or_none()
            if not session:
                return

            # å¦‚æœæŠ¥å‘Šå·²ç”Ÿæˆï¼Œé¿å…é‡å¤
            if session.report_ready:
                return

            # è·å–å¯¹è¯å†å²
            from backend.db.models import Message
            msg_result = await db.execute(
                select(Message)
                .where(Message.session_id == session_id)
                .order_by(Message.created_at.asc())
            )
            messages = msg_result.scalars().all()
            full_history = [
                {"role": m.role, "content": m.content} for m in messages
            ]

            # è°ƒç”¨ model2 ç”ŸæˆæŠ¥å‘Š
            report = await self.model2.final_report(
                full_history=full_history,
                mode=mode,
                topic_id=topic_id,
                trait_summary=trait_summary,
                trait_profile=trait_profile,
            )

            # æ›´æ–°æ•°æ®åº“
            session.report_ready = True
            session.opinion_report = report
            await db.commit()

        except Exception as e:
            print(f"[ERROR] åå°æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            await db.rollback()

    # ------------------------------------------------------
    # ä¸»æµå¼å…¥å£
    # ------------------------------------------------------
    async def stream_response(
        self,
        session_id: str,
        mode: int,
        topic_id: Optional[int],
        user_input: str,
        is_first: bool = False,
        force_end: bool = False,
        db: Optional[AsyncSession] = None,
        user_id: Optional[int] = None,
    ) -> AsyncGenerator[dict, None]:

        if db is None or user_id is None:
            raise ValueError("db and user_id are required")

        # è§„èŒƒåŒ– topic_id
        if topic_id is not None and isinstance(topic_id, str):
            try:
                topic_id = int(topic_id)
            except Exception:
                raise ValueError(f"Invalid topic_id: {topic_id}")

        # åŸºäºå½“å‰ç”¨æˆ·æ„é€  DB å†å²ç®¡ç†å™¨
        history_mgr = DatabaseHistoryManager(db=db, user_id=user_id)
        await history_mgr.ensure_session(session_id=session_id, mode=mode, topic_id=topic_id)

        # å½“å‰ç”¨æˆ·é•¿æœŸç‰¹è´¨
        trait_summary, trait_profile = await self._load_trait_context(db, user_id)

        # ç”¨æˆ·ä¸»åŠ¨ç»“æŸ
        if force_end:
            async for event in self._handle_final_outputs(
                session_id=session_id,
                mode=mode,
                topic_id=topic_id,
                force_end=True,
                history_mgr=history_mgr,
                db=db,
                user_id=user_id,
                trait_summary=trait_summary,
                trait_profile=trait_profile,
            ):
                yield event
            return

        # ------------------------------
        # model1 system prompt
        # ------------------------------
        system_prompt = load_prompt("model1/system.txt")

        if trait_summary:
            system_prompt += (
                "\n\n# ç”¨æˆ·é•¿æœŸç‰¹è´¨æ€»ç»“ï¼ˆä¾›ä½ å‚è€ƒï¼‰ï¼š\n"
                f"{trait_summary}"
            )

        assistant_text = ""

        # =======================================================
        # mode == 1ï¼ˆè¯é¢˜æµ‹è¯•ï¼‰
        # =======================================================
        if mode == 1:

            if topic_id is None:
                raise ValueError("mode1 requires topic_id")

            topic = next((t for t in TOPICS if t["id"] == topic_id), None)
            if topic is None:
                raise ValueError(f"Invalid topic_id: {topic_id}")

            # --------------------------
            # ç¬¬ä¸€è½®ï¼šæ¨¡å‹å…ˆè¯´
            # --------------------------
            if is_first:
                history = await history_mgr.get(session_id)
                mode1_intro = load_prompt("model1/mode1_intro.txt")

                system_prompt = (
                    system_prompt
                    + f"\n\n# æœ¬æ¬¡å¯¹è¯çš„ä¸»é¢˜æ˜¯ï¼š{topic['topic']}ï¼ˆè§‚å¿µæ ‡ç­¾ï¼š{topic['concept_tag']}ï¼‰ã€‚"
                    + mode1_intro
                )

                final_prompt = ""

                # ğŸ†• å…ˆæ”¶é›†å®Œæ•´è¾“å‡º
                async for chunk in self.llm.chat_stream(
                    system_prompt=system_prompt,
                    user_prompt=final_prompt,
                    history=history,
                ):
                    assistant_text += str(chunk)

                # ğŸ†• æ¸…æ´—åå†æµå¼è¾“å‡º
                visible_text = strip_control_markers(assistant_text)
                
                # é€å­—ç¬¦æµå¼è¾“å‡ºï¼ˆæ¨¡æ‹Ÿæ‰“å­—æœºæ•ˆæœï¼‰
                for char in visible_text:
                    yield {"type": "token", "content": char}
                
                await history_mgr.add(session_id, "assistant", visible_text)

                # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æƒ³é€€å‡º
                flags = parse_control_flags(assistant_text)
                if flags.user_want_to_quit:
                    yield {"type": "user_want_quit"}
                return

            # --------------------------
            # åç»­è½®ï¼šç”¨æˆ·å…ˆè¯´
            # --------------------------
            else:
                await history_mgr.add(session_id, "user", user_input)
                history = await history_mgr.get(session_id)

                # è°ƒç”¨ model2 åˆ†æ
                analysis = await self.model2.analyze(
                    session_history=history,
                    user_input=user_input,
                    mode=1,
                    topic_id=topic_id,
                    trait_summary=trait_summary,
                    trait_profile=trait_profile,
                )
                advice = analysis.get("advice", "")
                report_ready = analysis.get("signals", {}).get("report_ready", False)

                # å¦‚æœæŠ¥å‘Šå°±ç»ªï¼Œè§¦å‘åå°ç”Ÿæˆä»»åŠ¡
                if report_ready:
                    asyncio.create_task(
                        self._generate_report_background(
                            session_id=session_id,
                            mode=mode,
                            topic_id=topic_id,
                            db=db,
                            trait_summary=trait_summary,
                            trait_profile=trait_profile,
                        )
                    )
                    # å‘ŠçŸ¥ model1
                    advice += "\n\n[å†…éƒ¨æç¤º] è§‚å¿µå·²æ•æ‰å®Œæˆï¼Œè¯·åœ¨æœ¬æ¬¡å›å¤ä¸­è‡ªç„¶åœ°å‘ŠçŸ¥ç”¨æˆ·ï¼šä½ å·²ç»æˆåŠŸæ•æ‰åˆ°ä»–çš„è§‚å¿µï¼Œç¨åå¯ä»¥æŸ¥çœ‹åˆ†ææŠ¥å‘Šã€‚"

                final_prompt = (
                    "# æ¥è‡ªå†…éƒ¨æ¨¡å‹çš„å»ºè®®ï¼ˆç”¨æˆ·ä¸å¯è§ï¼‰ï¼š\n"
                    + advice
                    + "\n\n# ç”¨æˆ·çš„æœ€æ–°å›ç­”ï¼š\n"
                    + user_input
                )

                # ğŸ†• å…ˆæ”¶é›†å®Œæ•´è¾“å‡º
                async for chunk in self.llm.chat_stream(
                    system_prompt=system_prompt,
                    user_prompt=final_prompt,
                    history=history,
                ):
                    assistant_text += chunk

                # ğŸ†• æ¸…æ´—åå†æµå¼è¾“å‡º
                visible_text = strip_control_markers(assistant_text)
                
                # é€å­—ç¬¦æµå¼è¾“å‡º
                for char in visible_text:
                    yield {"type": "token", "content": char}

                await history_mgr.add(session_id, "assistant", visible_text)

                # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æƒ³é€€å‡º
                flags = parse_control_flags(assistant_text)
                if flags.user_want_to_quit:
                    yield {"type": "user_want_quit"}
                return

        # =======================================================
        # mode == 2ï¼ˆéšä¾¿èŠèŠï¼‰
        # =======================================================
        elif mode == 2:

            await history_mgr.add(session_id, "user", user_input)
            history = await history_mgr.get(session_id)

            # è°ƒç”¨ model2 åˆ†æ
            analysis = await self.model2.analyze(
                session_history=history,
                user_input=user_input,
                mode=2,
                topic_id=None,
                trait_summary=trait_summary,
                trait_profile=trait_profile,
            )
            advice = analysis.get("advice", "")
            report_ready = analysis.get("signals", {}).get("report_ready", False)

            # å¦‚æœæŠ¥å‘Šå°±ç»ªï¼Œè§¦å‘åå°ç”Ÿæˆä»»åŠ¡
            if report_ready:
                asyncio.create_task(
                    self._generate_report_background(
                        session_id=session_id,
                        mode=mode,
                        topic_id=None,
                        db=db,
                        trait_summary=trait_summary,
                        trait_profile=trait_profile,
                    )
                )
                # å‘ŠçŸ¥ model1
                advice += "\n\n[å†…éƒ¨æç¤º] è§‚å¿µå·²æ•æ‰å®Œæˆï¼Œè¯·åœ¨æœ¬æ¬¡å›å¤ä¸­è‡ªç„¶åœ°å‘ŠçŸ¥ç”¨æˆ·ï¼šä½ å·²ç»æˆåŠŸæ•æ‰åˆ°ä»–çš„è§‚å¿µï¼Œç¨åå¯ä»¥æŸ¥çœ‹åˆ†ææŠ¥å‘Šã€‚"

            mode2_intro = load_prompt("model1/mode2_intro.txt")
            system_prompt = system_prompt + "\n\n" + mode2_intro

            final_prompt = (
                "\n\n# æ¥è‡ªå†…éƒ¨æ¨¡å‹çš„å»ºè®®ï¼ˆç”¨æˆ·ä¸å¯è§ï¼‰ï¼š\n"
                + advice
                + "\n\n# ç”¨æˆ·çš„æœ€æ–°å›ç­”ï¼š\n"
                + user_input
            )

            # ğŸ†• å…ˆæ”¶é›†å®Œæ•´è¾“å‡º
            async for chunk in self.llm.chat_stream(
                system_prompt=system_prompt,
                user_prompt=final_prompt,
                history=history,
            ):
                assistant_text += chunk

            # ğŸ†• æ¸…æ´—åå†æµå¼è¾“å‡º
            visible_text = strip_control_markers(assistant_text)
            
            # é€å­—ç¬¦æµå¼è¾“å‡º
            for char in visible_text:
                yield {"type": "token", "content": char}

            await history_mgr.add(session_id, "assistant", visible_text)

            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æƒ³é€€å‡º
            flags = parse_control_flags(assistant_text)
            if flags.user_want_to_quit:
                yield {"type": "user_want_quit"}
            return

        else:
            raise ValueError(f"Unknown mode: {mode}")

    # =======================================================
    # æ ¼å¼åŒ–å†å² â†’ ä¸€å¥è¯æ€»ç»“
    # =======================================================
    def _format_history_for_summary(self, history: List[Dict]) -> str:
        lines = []
        for turn in history:
            role = "ç”¨æˆ·" if turn.get("role") == "user" else "åŠ©æ‰‹"
            lines.append(f"{role}ï¼š{turn.get('content', '')}")
        return "\n".join(lines).strip()

    # =======================================================
    # æ”¶å°¾é€»è¾‘ï¼šsummary + traitsï¼ˆä¸å†ç”Ÿæˆ reportï¼‰
    # =======================================================
    async def _handle_final_outputs(
        self,
        session_id: str,
        mode: int,
        topic_id: Optional[int],
        force_end: bool,
        history_mgr: DatabaseHistoryManager,
        db: AsyncSession,
        user_id: int,
        trait_summary: str,
        trait_profile: str,
    ) -> AsyncGenerator[dict, None]:

        # 1. å–å†å²
        full_history = await history_mgr.get(session_id)

        # 2. model1 summary
        summary_prompt = (
            "è¯·æ ¹æ®ä»¥ä¸‹å®Œæ•´å¯¹è¯ï¼Œç”Ÿæˆä¸€å¥è¯æ€»ç»“ï¼ˆé¢å‘ç”¨æˆ·ï¼Œå¯ç›´æ¥å±•ç¤ºï¼‰ï¼š\n\n"
            + self._format_history_for_summary(full_history)
        )

        model1_summary = ""
        async for chunk in self.llm.chat_stream(
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæ“…é•¿å¯¹å¯¹è¯è¿›è¡Œé«˜åº¦æ¦‚æ‹¬çš„åŠ©æ‰‹ã€‚",
            user_prompt=summary_prompt,
            history=[],
        ):
            model1_summary += chunk

        model1_summary = strip_control_markers(model1_summary).strip()

        # 3. model3ï¼šæ›´æ–°ç‰¹è´¨ï¼ˆåªç”¨æœ¬ sessionï¼‰
        trait_data = await self.model3.update_traits({session_id: full_history})
        new_trait_summary = trait_data.get("summary", "")
        new_full_report = trait_data.get("full_report", "")

        # 4. å†™ TraitProfile
        result = await db.execute(
            select(TraitProfile).where(TraitProfile.user_id == user_id)
        )
        profile = result.scalar_one_or_none()

        if profile is None:
            profile = TraitProfile(
                user_id=user_id,
                summary=new_trait_summary,
                full_report=new_full_report,
            )
            db.add(profile)
        else:
            profile.summary = new_trait_summary
            profile.full_report = new_full_report

        await db.commit()
        
        # 5. æ ‡è®° session å®Œæˆ
        session = await db.execute(select(Session).where(Session.id == session_id))
        session = session.scalar_one_or_none()
        if session:
            session.is_completed = True
            db.add(session)
            await db.commit()

        # 6. è¾“å‡ºæœ€ç»ˆäº‹ä»¶
        yield {
            "type": "end",
            "summary": model1_summary,
            "trait_summary": new_trait_summary,
            "full_dialogue": full_history,
        }