# backend/services/chat_service.py

import asyncio
import json
from typing import AsyncGenerator, Optional, List, Dict

from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

# âŒ å·²åˆ é™¤æ—§ç‰ˆå¼•ç”¨: from backend.data.topics import TOPICS
from backend.llm_client.base import LLMClient
from backend.utils.prompt_loader import load_prompt
from backend.utils.text_tools import strip_control_markers, parse_control_flags
from backend.services.model2_service import Model2Service
from backend.services.model3_service import Model3Service
from backend.services.db_history_manager import DatabaseHistoryManager
from backend.db.models import TraitProfile, Session
from backend.db.crud import topic as topic_crud


class ChatService:
    """
    ChatServiceï¼šè´Ÿè´£ä¸‰å±‚é€»è¾‘çš„ç¼–æ’ä¸å¯¹æ¥ï¼š
    - model1ï¼šå¯¹è¯ï¼ˆæœ¬ç±»å†…é€šè¿‡ llm å®ç°ï¼‰
    - model2ï¼šè§‚å¿µåˆ†æ + å¯¹è¯å»ºè®®ï¼ˆModel2Serviceï¼‰
    - model3ï¼šç‰¹è´¨åˆ†æï¼ˆModel3Serviceï¼‰

    å¯¹å¤–æš´éœ²ä¸€ä¸ªç»Ÿä¸€çš„æµå¼æ¥å£ï¼š
    async def stream_response(...) -> AsyncGenerator[dict, None]
    """

    def __init__(self, llm: LLMClient):
        self.llm = llm
        self.model2 = Model2Service(llm)
        self.model3 = Model3Service(llm)

    # ------------------------------------------------------
    # è¯»å–ç”¨æˆ·å½“å‰ trait
    # ------------------------------------------------------
    async def _load_trait_context(
        self,
        db: AsyncSession,
        user_id: int,
    ) -> tuple[str, str]:
        """
        ä» TraitProfile è¡¨ä¸­åŠ è½½è¯¥ç”¨æˆ·çš„é•¿æœŸç‰¹è´¨ summary å’Œ full_reportã€‚
        å¦‚æœæ²¡æœ‰è®°å½•ï¼Œåˆ™è¿”å› ('', '')ã€‚
        """
        result = await db.execute(
            select(TraitProfile).where(TraitProfile.user_id == user_id)
        )
        profile = result.scalar_one_or_none()

        if not profile:
            return "", ""

        return str(profile.summary or ""), str(profile.full_report or "")

    # ------------------------------------------------------
    # ğŸ†• v1.4: è·å–è¯é¢˜æç¤ºè¯ï¼ˆä»Sessionå¿«ç…§æˆ–æ•°æ®åº“ï¼‰
    # ------------------------------------------------------
    async def _get_topic_prompt(
        self,
        db: AsyncSession,
        session: Session,
        topic_id: Optional[int]
    ) -> tuple[Optional[str], Optional[str], Optional[str], Optional[List[str]]]:
        """
        è·å–è¯é¢˜çš„æç¤ºè¯ã€æ ‡é¢˜å’Œæ ‡ç­¾

        ä¼˜å…ˆçº§:
        1. ä» session.topic_prompt è¯»å–ï¼ˆå¿«ç…§ï¼‰
        2. ä»æ•°æ®åº“æŸ¥è¯¢è¯é¢˜ï¼ˆå¦‚æœsessionä¸­æ²¡æœ‰å¿«ç…§ï¼‰

        è¿”å›:
            (prompt, title, concept_tag, tags_list)

        æ³¨æ„: å·²åˆ é™¤æ—§ç‰ˆTOPICSå­—å…¸é™çº§é€»è¾‘
        """
        # 1. ä¼˜å…ˆä½¿ç”¨ Session çš„å¿«ç…§
        if session.topic_prompt:
            tags = json.loads(session.topic_tags_snapshot) if session.topic_tags_snapshot else []
            return (session.topic_prompt, session.topic_title, None, tags)

        # 2. æ— å¿«ç…§ï¼ˆæ—§ session æˆ–é¦–æ¬¡ï¼‰â†’ æŸ¥æ•°æ®åº“å¹¶å†™å…¥å¿«ç…§
        if topic_id:
            topic = await topic_crud.get_topic_by_id(db, topic_id, include_inactive=True)
            if topic:
                tags_list = [tt.tag.name for tt in topic.tags]
                # å†™å¿«ç…§
                session.topic_prompt = topic.prompt
                session.topic_title = topic.title
                session.topic_tags_snapshot = json.dumps(tags_list, ensure_ascii=False)
                session.topic_version = topic.updated_at
                await db.commit()
                return (topic.prompt, topic.title, None, tags_list)

        return None, None, None, []

    # ------------------------------------------------------
    # âœ… ä¿®å¤ï¼šåå°å¼‚æ­¥ç”ŸæˆæŠ¥å‘Šï¼ˆä½¿ç”¨ç‹¬ç«‹ db sessionï¼‰
    # ------------------------------------------------------
    async def _generate_report_background(
        self,
        session_id: str,
        mode: int,
        topic_id: Optional[int],
        # âœ… ä¸å†æ¥æ”¶å¤–éƒ¨ dbï¼Œæ”¹ä¸ºåœ¨æ–¹æ³•å†…éƒ¨åˆ›å»ºç‹¬ç«‹ session
        # åŸå› ï¼šasyncio.create_task ä¸­ä½¿ç”¨è¯·æ±‚çº§ db session ä¼šå¯¼è‡´
        # è¯·æ±‚ç»“æŸå session è¢«å…³é—­ï¼Œåå°ä»»åŠ¡é™é»˜å¤±è´¥ï¼ŒæŠ¥å‘Šæ°¸è¿œæ— æ³•ç”Ÿæˆ
        trait_summary: str,
        trait_profile: str,
    ):
        """
        åå°ä»»åŠ¡ï¼šç”Ÿæˆè§‚å¿µæŠ¥å‘Šå¹¶æ›´æ–°æ•°æ®åº“ã€‚
        ä½¿ç”¨ç‹¬ç«‹çš„ db sessionï¼Œç”Ÿå‘½å‘¨æœŸç”±æœ¬ä»»åŠ¡è‡ªå·±ç®¡ç†ã€‚
        """
        from backend.db.database import get_sessionmaker
        from backend.db.models import Message

        SessionLocal = get_sessionmaker()
        async with SessionLocal() as db:
            try:
                # è·å– session è®°å½•
                result = await db.execute(
                    select(Session).where(Session.id == session_id)
                )
                session = result.scalar_one_or_none()
                if not session:
                    return

                # å¦‚æœæŠ¥å‘Šå·²ç”Ÿæˆï¼Œé¿å…é‡å¤
                if session.report_ready:
                    return

                # è·å–å¯¹è¯å†å²
                msg_result = await db.execute(
                    select(Message)
                    .where(Message.session_id == session_id)
                    .order_by(Message.created_at.asc())
                )
                messages = msg_result.scalars().all()
                full_history = [
                    {"role": m.role, "content": m.content} for m in messages
                ]

                # ğŸ†• v1.4: è·å–è¯é¢˜ä¿¡æ¯ï¼ˆç”¨äºæŠ¥å‘Šç”Ÿæˆï¼‰
                _, topic_title, _, topic_tags = await self._get_topic_prompt(
                    db, session, topic_id
                )

                # è°ƒç”¨ model2 ç”ŸæˆæŠ¥å‘Š
                report = await self.model2.final_report(
                    full_history=full_history,
                    mode=mode,
                    topic_id=topic_id,
                    topic_title=topic_title,
                    topic_tags=topic_tags or [],
                    trait_summary=trait_summary,
                    trait_profile=trait_profile,
                )

                # æ›´æ–°æ•°æ®åº“
                session.report_ready = True
                session.opinion_report = report
                await db.commit()

            except Exception as e:
                print(f"[ERROR] åå°æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
                await db.rollback()

    # ------------------------------------------------------
    # ä¸»æµå¼å…¥å£
    # ------------------------------------------------------
    async def stream_response(
        self,
        session_id: str,
        mode: int,
        topic_id: Optional[int],
        user_input: str,
        is_first: bool = False,
        force_end: bool = False,
        db: Optional[AsyncSession] = None,
        user_id: Optional[int] = None,
    ) -> AsyncGenerator[dict, None]:

        if db is None or user_id is None:
            raise ValueError("db and user_id are required")

        # è§„èŒƒåŒ– topic_id
        if topic_id is not None and isinstance(topic_id, str):
            try:
                topic_id = int(topic_id)
            except Exception:
                raise ValueError(f"Invalid topic_id: {topic_id}")

        # åŸºäºå½“å‰ç”¨æˆ·æ„é€  DB å†å²ç®¡ç†å™¨
        history_mgr = DatabaseHistoryManager(db=db, user_id=user_id)
        session = await history_mgr.ensure_session(
            session_id=session_id,
            mode=mode,
            topic_id=topic_id
        )

        # ğŸ†• v1.4: å¦‚æœæ˜¯æ–°Sessionä¸”æœ‰topic_idï¼Œè¿›è¡Œå®Œæ•´å¿«ç…§
        if session.topic_prompt is None and topic_id is not None:
            topic = await topic_crud.get_topic_by_id(db, topic_id, include_inactive=False)
            if topic:
                session.topic_prompt = topic.prompt
                session.topic_title = topic.title
                session.topic_tags_snapshot = json.dumps(
                    [tt.tag.name for tt in topic.tags], ensure_ascii=False
                )
                session.topic_version = topic.updated_at
                await db.commit()

        """
        æ­¤å¤„æ£€æµ‹è¯é¢˜æ˜¯å¦å·²æ›´æ–°çš„åŠŸèƒ½å¾…å¼€å‘
        # å¯é€‰ï¼šæ£€æŸ¥è¯é¢˜æ˜¯å¦æœ‰æ›´æ–°
        if session.topic_version and topic_id:
            topic = await topic_crud.get_topic_by_id(db, topic_id, include_inactive=True)
            if topic and topic.updated_at > session.topic_version:
                yield {"type": "topic_updated", "content": "è¯¥è¯é¢˜å·²è¢«ä½œè€…æ›´æ–°ï¼Œæ˜¯å¦è¦ä½¿ç”¨æ–°ç‰ˆæœ¬ï¼Ÿ"}
                # å‰ç«¯å±•ç¤ºé€šçŸ¥ï¼Œç”¨æˆ·ç¡®è®¤åè°ƒç”¨ä¸€ä¸ªåˆ·æ–°å¿«ç…§çš„æ¥å£
        """

        # å½“å‰ç”¨æˆ·é•¿æœŸç‰¹è´¨
        trait_summary, trait_profile = await self._load_trait_context(db, user_id)

        # ç”¨æˆ·ä¸»åŠ¨ç»“æŸ
        if force_end:
            async for event in self._handle_final_outputs(
                session_id=session_id,
                mode=mode,
                topic_id=topic_id,
                force_end=True,
                history_mgr=history_mgr,
                db=db,
                user_id=user_id,
                trait_summary=trait_summary,
                trait_profile=trait_profile,
            ):
                yield event
            return

        # =======================================================
        # mode == 1ï¼ˆè¯é¢˜æµ‹è¯•ï¼‰
        # =======================================================
        if mode == 1:

            # è·å–è¯é¢˜ä¿¡æ¯
            topic_prompt, topic_title, _, topic_tags = await self._get_topic_prompt(
                db, session, topic_id
            )

            if not topic_prompt:
                raise ValueError(f"è¯é¢˜ ID {topic_id} ä¸å­˜åœ¨æˆ–æœªæ‰¾åˆ°æç¤ºè¯")

            # åŠ è½½ model1 åŸºç¡€ prompt
            base_model1 = load_prompt("model1/system.txt")
            system_prompt = base_model1 + "\n\n" + topic_prompt

            assistant_text = ""

            # --------------------------
            # é¦–è½®ï¼šæœºå™¨äººå…ˆä¸»åŠ¨è¯´è¯
            # --------------------------
            if is_first:
                first_prompt = load_prompt("model1/mode1_intro.txt")
                final_prompt = (
                    "# å†…éƒ¨æç¤ºï¼ˆç”¨æˆ·ä¸å¯è§ï¼‰ï¼š\n"
                    + first_prompt
                    + "\n\nè¯·æ ¹æ®è¯é¢˜ï¼Œç”Ÿæˆä½ çš„ç¬¬ä¸€å¥è¯ã€‚"
                )

                # å…ˆæ”¶é›†å®Œæ•´è¾“å‡º
                async for chunk in self.llm.chat_stream(
                    system_prompt=system_prompt,
                    user_prompt=final_prompt,
                    history=[],
                ):
                    assistant_text += chunk

                # æ¸…æ´—åå†æµå¼è¾“å‡º
                visible_text = strip_control_markers(assistant_text)

                # é€å­—ç¬¦æµå¼è¾“å‡º
                for char in visible_text:
                    yield {"type": "token", "content": char}

                await history_mgr.add(session_id, "assistant", visible_text)

                # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æƒ³é€€å‡º
                flags = parse_control_flags(assistant_text)
                if flags.user_want_to_quit:
                    yield {"type": "user_want_quit"}
                return

            # --------------------------
            # åç»­è½®ï¼šç”¨æˆ·å…ˆè¯´
            # --------------------------
            else:
                await history_mgr.add(session_id, "user", user_input)
                history = await history_mgr.get(session_id)

                # ğŸ†• v1.4: è°ƒç”¨ model2 åˆ†æï¼ˆä¼ å…¥è¯é¢˜å…ƒæ•°æ®ï¼‰
                analysis = await self.model2.analyze(
                    session_history=history,
                    user_input=user_input,
                    mode=1,
                    topic_id=topic_id,
                    topic_title=topic_title,
                    topic_tags=topic_tags or [],
                    trait_summary=trait_summary,
                    trait_profile=trait_profile,
                )
                advice = analysis.get("advice", "")
                report_ready = analysis.get("signals", {}).get("report_ready", False)

                # å¦‚æœæŠ¥å‘Šå°±ç»ªï¼Œè§¦å‘åå°ç”Ÿæˆä»»åŠ¡
                if report_ready:
                    # âœ… ä¿®å¤ï¼šå»æ‰ db=dbï¼Œæ”¹ç”± _generate_report_background è‡ªå»º session
                    asyncio.create_task(
                        self._generate_report_background(
                            session_id=session_id,
                            mode=mode,
                            topic_id=topic_id,
                            trait_summary=trait_summary,
                            trait_profile=trait_profile,
                        )
                    )
                    # å‘ŠçŸ¥ model1
                    advice += "\n\n[å†…éƒ¨æç¤º] è§‚å¿µå·²æ•æ‰å®Œæˆï¼Œè¯·åœ¨æœ¬æ¬¡å›å¤ä¸­è‡ªç„¶åœ°å‘ŠçŸ¥ç”¨æˆ·ï¼šä½ å·²ç»æˆåŠŸæ•æ‰åˆ°ä»–çš„è§‚å¿µï¼Œç¨åå¯ä»¥æŸ¥çœ‹åˆ†ææŠ¥å‘Šã€‚"

                final_prompt = (
                    "# æ¥è‡ªå†…éƒ¨æ¨¡å‹çš„å»ºè®®ï¼ˆç”¨æˆ·ä¸å¯è§ï¼‰ï¼š\n"
                    + advice
                    + "\n\n# ç”¨æˆ·çš„æœ€æ–°å›ç­”ï¼š\n"
                    + user_input
                )

                # å…ˆæ”¶é›†å®Œæ•´è¾“å‡º
                async for chunk in self.llm.chat_stream(
                    system_prompt=system_prompt,
                    user_prompt=final_prompt,
                    history=history,
                ):
                    assistant_text += chunk

                # æ¸…æ´—åå†æµå¼è¾“å‡º
                visible_text = strip_control_markers(assistant_text)

                # é€å­—ç¬¦æµå¼è¾“å‡º
                for char in visible_text:
                    yield {"type": "token", "content": char}

                await history_mgr.add(session_id, "assistant", visible_text)

                # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æƒ³é€€å‡º
                flags = parse_control_flags(assistant_text)
                if flags.user_want_to_quit:
                    yield {"type": "user_want_quit"}
                return

        # =======================================================
        # mode == 2ï¼ˆéšä¾¿èŠèŠï¼‰
        # =======================================================
        elif mode == 2:

            await history_mgr.add(session_id, "user", user_input)
            history = await history_mgr.get(session_id)

            # è°ƒç”¨ model2 åˆ†æ
            analysis = await self.model2.analyze(
                session_history=history,
                user_input=user_input,
                mode=2,
                topic_id=None,
                topic_title=None,
                topic_tags=[],
                trait_summary=trait_summary,
                trait_profile=trait_profile,
            )
            advice = analysis.get("advice", "")
            report_ready = analysis.get("signals", {}).get("report_ready", False)

            # å¦‚æœæŠ¥å‘Šå°±ç»ªï¼Œè§¦å‘åå°ç”Ÿæˆä»»åŠ¡
            if report_ready:
                # âœ… ä¿®å¤ï¼šå»æ‰ db=dbï¼Œæ”¹ç”± _generate_report_background è‡ªå»º session
                asyncio.create_task(
                    self._generate_report_background(
                        session_id=session_id,
                        mode=mode,
                        topic_id=None,
                        trait_summary=trait_summary,
                        trait_profile=trait_profile,
                    )
                )
                # å‘ŠçŸ¥ model1
                advice += "\n\n[å†…éƒ¨æç¤º] è§‚å¿µå·²æ•æ‰å®Œæˆï¼Œè¯·åœ¨æœ¬æ¬¡å›å¤ä¸­è‡ªç„¶åœ°å‘ŠçŸ¥ç”¨æˆ·ï¼šä½ å·²ç»æˆåŠŸæ•æ‰åˆ°ä»–çš„è§‚å¿µï¼Œç¨åå¯ä»¥æŸ¥çœ‹åˆ†ææŠ¥å‘Šã€‚"

            base_model1 = load_prompt("model1/base.txt")
            system_prompt = base_model1
            mode2_intro = load_prompt("model1/mode2_intro.txt")
            system_prompt = system_prompt + "\n\n" + mode2_intro

            final_prompt = (
                "\n\n# æ¥è‡ªå†…éƒ¨æ¨¡å‹çš„å»ºè®®ï¼ˆç”¨æˆ·ä¸å¯è§ï¼‰ï¼š\n"
                + advice
                + "\n\n# ç”¨æˆ·çš„æœ€æ–°å›ç­”ï¼š\n"
                + user_input
            )

            assistant_text = ""
            # å…ˆæ”¶é›†å®Œæ•´è¾“å‡º
            async for chunk in self.llm.chat_stream(
                system_prompt=system_prompt,
                user_prompt=final_prompt,
                history=history,
            ):
                assistant_text += chunk

            # æ¸…æ´—åå†æµå¼è¾“å‡º
            visible_text = strip_control_markers(assistant_text)

            # é€å­—ç¬¦æµå¼è¾“å‡º
            for char in visible_text:
                yield {"type": "token", "content": char}

            await history_mgr.add(session_id, "assistant", visible_text)

            # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æƒ³é€€å‡º
            flags = parse_control_flags(assistant_text)
            if flags.user_want_to_quit:
                yield {"type": "user_want_quit"}
            return

        else:
            raise ValueError(f"Unknown mode: {mode}")

    # =======================================================
    # æ ¼å¼åŒ–å†å² â†’ ä¸€å¥è¯æ€»ç»“
    # =======================================================
    def _format_history_for_summary(self, history: List[Dict]) -> str:
        lines = []
        for turn in history:
            role = "ç”¨æˆ·" if turn.get("role") == "user" else "åŠ©æ‰‹"
            lines.append(f"{role}ï¼š{turn.get('content', '')}")
        return "\n".join(lines).strip()

    # =======================================================
    # æ”¶å°¾é€»è¾‘ï¼šsummary + traits
    # =======================================================
    async def _handle_final_outputs(
        self,
        session_id: str,
        mode: int,
        topic_id: Optional[int],
        force_end: bool,
        history_mgr: DatabaseHistoryManager,
        db: AsyncSession,
        user_id: int,
        trait_summary: str,
        trait_profile: str,
    ) -> AsyncGenerator[dict, None]:

        # 1. è·å–å†å²
        full_history = await history_mgr.get(session_id)

        # 2. model1 summary
        summary_prompt = (
            "è¯·æ ¹æ®ä»¥ä¸‹å®Œæ•´å¯¹è¯ï¼Œç”Ÿæˆä¸€å¥è¯æ€»ç»“ï¼ˆé¢å‘ç”¨æˆ·ï¼Œå¯ç›´æ¥å±•ç¤ºï¼‰ï¼š\n\n"
            + self._format_history_for_summary(full_history)
        )

        model1_summary = ""
        async for chunk in self.llm.chat_stream(
            system_prompt="ä½ æ˜¯ä¸€ä¸ªæ“…é•¿å¯¹å¯¹è¯è¿›è¡Œé«˜åº¦æ¦‚æ‹¬çš„åŠ©æ‰‹ã€‚",
            user_prompt=summary_prompt,
            history=[],
        ):
            model1_summary += chunk

        model1_summary = strip_control_markers(model1_summary).strip()

        # 3. model3ï¼šæ›´æ–°ç‰¹è´¨ï¼ˆåªç”¨æœ¬ sessionï¼‰
        trait_data = await self.model3.update_traits({session_id: full_history})
        new_trait_summary = trait_data.get("summary", "")
        new_full_report = trait_data.get("full_report", "")

        # 4. å†™ TraitProfile
        result = await db.execute(
            select(TraitProfile).where(TraitProfile.user_id == user_id)
        )
        profile = result.scalar_one_or_none()

        if profile is None:
            profile = TraitProfile(
                user_id=user_id,
                summary=new_trait_summary,
                full_report=new_full_report,
            )
            db.add(profile)
        else:
            profile.summary = new_trait_summary
            profile.full_report = new_full_report

        await db.commit()

        # 5. æ ‡è®° session å®Œæˆ
        session = await db.execute(select(Session).where(Session.id == session_id))
        session = session.scalar_one_or_none()
        if session:
            session.is_completed = True
            db.add(session)
            await db.commit()

        # 6. è¾“å‡ºæœ€ç»ˆäº‹ä»¶
        yield {
            "type": "end",
            "summary": model1_summary,
            "trait_summary": new_trait_summary,
            "full_dialogue": full_history,
        }
